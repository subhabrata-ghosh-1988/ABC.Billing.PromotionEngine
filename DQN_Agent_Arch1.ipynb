{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "DQN_Agent_Arch1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/ABC.Billing.PromotionEngine/blob/master/DQN_Agent_Arch1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Ij1vxZQ6Ix"
      },
      "source": [
        "### Cab-Driver Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV-YeUj6Q9B3",
        "outputId": "759b837d-ac1d-4a52-85c7-173f995e7e30"
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8s99idSRm7H"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/Env.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('Env.py','w') as f:\n",
        "     f.write(r.text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWfJ0Z5QQ6JE"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import deque\n",
        "import collections\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# for plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the environment\n",
        "from Env import CabDriver"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsxhZZvsQ6JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56f9751-bd68-4cd0-a6ab-7ebe4961669d"
      },
      "source": [
        "!wget https://github.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/raw/main/TM.npy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-04 00:01:30--  https://github.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/raw/main/TM.npy\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/TM.npy [following]\n",
            "--2021-05-04 00:01:30--  https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/TM.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33728 (33K) [application/octet-stream]\n",
            "Saving to: ‘TM.npy’\n",
            "\n",
            "TM.npy              100%[===================>]  32.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-04 00:01:31 (114 MB/s) - ‘TM.npy’ saved [33728/33728]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWx99N1vmze6"
      },
      "source": [
        "#tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blTVtc4MQ6JK"
      },
      "source": [
        "#### Defining Time Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGCYPABJQ6JM"
      },
      "source": [
        "# Loading the time matrix provided\n",
        "Time_matrix = np.load(\"TM.npy\", allow_pickle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573Lxpckb6dO"
      },
      "source": [
        "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3OrekPlbwp4",
        "outputId": "d2013ff5-b86c-4ad4-cf3a-aefe15d5182f"
      },
      "source": [
        "print(type(Time_matrix))\n",
        "print(Time_matrix.max())\n",
        "print(Time_matrix.min())\n",
        "print(Time_matrix.mean())\n",
        "print(Time_matrix.var())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "11.0\n",
            "0.0\n",
            "3.0542857142857143\n",
            "7.93705306122449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cBe6IZqcDbF"
      },
      "source": [
        "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by 1 day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJvDTRo3Q6JN"
      },
      "source": [
        "#### Tracking the state-action pairs for checking convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQnwpFsrp3Tv"
      },
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUeWmqVJQ6JS"
      },
      "source": [
        "### Agent Class\n",
        "\n",
        "If you are using this framework, you need to fill the following to complete the following code block:\n",
        "1. State and Action Size\n",
        "2. Hyperparameters\n",
        "3. Create a neural-network model in function 'build_model()'\n",
        "4. Define epsilon-greedy strategy in function 'get_action()'\n",
        "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
        "6. Complete the 'train_model()' function with following logic:\n",
        "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
        "      - Initialise your input and output batch for training the model\n",
        "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
        "      - Get Q(s', a) values from the last trained model\n",
        "      - Update the input batch as your encoded state and output batch as your Q-values\n",
        "      - Then fit your DQN model using the updated input and output batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCagoCqNQ6JU"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_decay = -0.0005 #for 15k\n",
        "        #self.epsilon_decay = -0.00015 #for 20k\n",
        "        self.epsilon_min = 0.00001\n",
        "        \n",
        "        self.batch_size = 32\n",
        "\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # Initialize the value of the states tracked\n",
        "        self.states_tracked = []\n",
        "        \n",
        "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
        "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
        "\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Function that takes in the agent and constructs the network\n",
        "        to train it\n",
        "        @return model\n",
        "        @params agent\n",
        "        \"\"\"\n",
        "        input_shape = self.state_size\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets       \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        model.summary\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state, possible_actions_index, actions):\n",
        "        \"\"\"\n",
        "        get action in a state according to an epsilon-greedy approach\n",
        "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
        "        \"\"\"        \n",
        "        # get action from model using epsilon-greedy policy\n",
        "        # Decay in ε after each episode       \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # explore: choose a random action from the ride requests\n",
        "            return random.choice(possible_actions_index)\n",
        "        else:\n",
        "            # choose the action with the highest q(s, a)\n",
        "            # the first index corresponds to the batch size, so\n",
        "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
        "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
        "\n",
        "            # Use the model to predict the Q_values.\n",
        "            q_value = self.model.predict(state)\n",
        "\n",
        "            # truncate the array to only those actions that are part of the ride  requests.\n",
        "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
        "\n",
        "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
        "\n",
        "    def append_sample(self, state, action_index, reward, next_state, done):\n",
        "        \"\"\"appends the new agent run output to replay buffer\"\"\"\n",
        "        self.memory.append((state, action_index, reward, next_state, done))\n",
        "        \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        \"\"\" \n",
        "        Function to train the model on eacg step run.\n",
        "        Picks the random memory events according to batch size and \n",
        "        runs it through the network to train it.\n",
        "        \"\"\"\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            # Sample batch from the memory\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            # initialise two matrices - update_input and update_output\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            actions, rewards, done = [], [], []\n",
        "\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
        "                update_input[i] = env.state_encod_arch1(state)     \n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                update_output[i] = env.state_encod_arch1(next_state)\n",
        "                done.append(done_boolean)\n",
        "\n",
        "            # predict the target q-values from states s\n",
        "            target = self.model.predict(update_input)\n",
        "            # target for q-network\n",
        "            target_qval = self.model.predict(update_output)\n",
        "\n",
        "\n",
        "            # update the target values\n",
        "            for i in range(self.batch_size):\n",
        "                if done[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else: # non-terminal state\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "            # model fit\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "            \n",
        "    def save_tracking_states(self):\n",
        "        # Use the model to predict the q_value of the state we are tacking.\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        \n",
        "        # Grab the q_value of the action index that we are tracking.\n",
        "        self.states_tracked.append(q_value[0][2])\n",
        "        \n",
        "    def save_test_states(self):\n",
        "        # Use the model to predict the q_value of the state we are tacking.\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        \n",
        "        # Grab the q_value of the action index that we are tracking.\n",
        "        self.states_test.append(q_value[0][2])\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRJU7n8Q6JW"
      },
      "source": [
        "### DQN block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WTQx9gYeNTQ"
      },
      "source": [
        "for episode in range(Episodes):\n",
        "\n",
        "    # Write code here\n",
        "    # Call the environment\n",
        "    # Call all the initialised variables of the environment\n",
        "    \n",
        "\n",
        "    #Call the DQN agent\n",
        "    \n",
        "    \n",
        "    while !terminal_state:\n",
        "        \n",
        "        # Write your code here\n",
        "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
        "        # 2. Evaluate your reward and next state\n",
        "        # 3. Append the experience to the memory\n",
        "        # 4. Train the model by calling function agent.train_model\n",
        "        # 5. Keep a track of rewards, Q-values, loss\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKOKQKCceIHB"
      },
      "source": [
        "episode_time = 24*30 #30 days before which car has to be recharged\n",
        "n_episodes = 15000\n",
        "m = 5\n",
        "t = 24\n",
        "d = 7\n",
        "\n",
        "# Invoke Env class\n",
        "env = CabDriver()\n",
        "action_space, state_space, state = env.reset()\n",
        "\n",
        "# Set up state and action sizes.\n",
        "state_size = m+t+d\n",
        "action_size = len(action_space)\n",
        "\n",
        "# Invoke agent class\n",
        "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
        "\n",
        "# to store rewards in each episode\n",
        "rewards_per_episode, episodes = [], []\n",
        "# Rewards for state [0,0,0] being tracked.\n",
        "rewards_init_state = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LjiJlpeiJU"
      },
      "source": [
        "#### Run the episodes, build up replay buffer and train the model.\n",
        "Note:\n",
        "The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n",
        "The init state is randomly picked from the state space for each episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUED4EPpQ6JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73270814-373f-4410-eb21-796a0dbcde5a"
      },
      "source": [
        "start_time = time.time()\n",
        "score_tracked = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "    track_reward = False\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    env = CabDriver()\n",
        "    action_space, state_space, state = env.reset()\n",
        "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
        "    initial_state = env.state_init\n",
        "\n",
        "\n",
        "    total_time = 0  # Total time driver rode in this episode\n",
        "    while not done:\n",
        "        # 1. Get a list of the ride requests driver got.\n",
        "        possible_actions_indices, actions = env.requests(state)\n",
        "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
        "        action = agent.get_action(state, possible_actions_indices, actions)\n",
        "\n",
        "        # 3. Evaluate your reward and next state\n",
        "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
        "        # 4. Total time driver rode in this episode\n",
        "        total_time += step_time\n",
        "        if (total_time > episode_time):\n",
        "            # if ride does not complete in stipu;ated time skip\n",
        "            # it and move to next episode.\n",
        "            done = True\n",
        "        else:\n",
        "            # 5. Append the experience to the memory\n",
        "            agent.append_sample(state, action, reward, next_state, done)\n",
        "            # 6. Train the model by calling function agent.train_model\n",
        "            agent.train_model()\n",
        "            # 7. Keep a track of rewards, Q-values, loss\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "    \n",
        "\n",
        "    # epsilon decay\n",
        "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
        "\n",
        "    # every 10 episodes:\n",
        "    if ((episode + 1) % 10 == 0):\n",
        "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon, total_time))\n",
        "    # Save the Q_value of the state, action pair we are tracking\n",
        "    if ((episode + 1) % 5 == 0):\n",
        "        agent.save_tracking_states()\n",
        "\n",
        "    # Total rewards per episode\n",
        "    score_tracked.append(score)\n",
        "\n",
        "    if(episode % 1000 == 0):\n",
        "        print(\"Saving Model {}\".format(episode))\n",
        "        agent.save(name=\"model_weights.h5\")\n",
        "    \n",
        "elapsed_time = time.time() - start_time\n",
        "print(elapsed_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving Model 0\n",
            "episode 9, reward -387.0, memory_length 1359, epsilon 0.9955001547284723 total_time 729.0\n",
            "episode 19, reward -24.0, memory_length 2000, epsilon 0.9905350769930761 total_time 724.0\n",
            "episode 29, reward -634.0, memory_length 2000, epsilon 0.9855947626861951 total_time 724.0\n",
            "episode 39, reward -270.0, memory_length 2000, epsilon 0.9806790882997144 total_time 727.0\n",
            "episode 49, reward -274.0, memory_length 2000, epsilon 0.9757879309415182 total_time 724.0\n",
            "episode 59, reward -126.0, memory_length 2000, epsilon 0.9709211683324178 total_time 726.0\n",
            "episode 69, reward -69.0, memory_length 2000, epsilon 0.9660786788030947 total_time 725.0\n",
            "episode 79, reward -259.0, memory_length 2000, epsilon 0.9612603412910584 total_time 724.0\n",
            "episode 89, reward -228.0, memory_length 2000, epsilon 0.9564660353376199 total_time 722.0\n",
            "episode 99, reward -45.0, memory_length 2000, epsilon 0.9516956410848808 total_time 723.0\n",
            "episode 109, reward -407.0, memory_length 2000, epsilon 0.9469490392727365 total_time 723.0\n",
            "episode 119, reward -2.0, memory_length 2000, epsilon 0.9422261112358942 total_time 726.0\n",
            "episode 129, reward 50.0, memory_length 2000, epsilon 0.9375267389009072 total_time 722.0\n",
            "episode 139, reward -286.0, memory_length 2000, epsilon 0.9328508047832221 total_time 722.0\n",
            "episode 149, reward -179.0, memory_length 2000, epsilon 0.9281981919842428 total_time 722.0\n",
            "episode 159, reward 109.0, memory_length 2000, epsilon 0.9235687841884068 total_time 724.0\n",
            "episode 169, reward -66.0, memory_length 2000, epsilon 0.918962465660278 total_time 728.0\n",
            "episode 179, reward -144.0, memory_length 2000, epsilon 0.9143791212416534 total_time 722.0\n",
            "episode 189, reward -183.0, memory_length 2000, epsilon 0.9098186363486838 total_time 726.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J34clOaAQ6JY"
      },
      "source": [
        "### Tracking Convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB5JiWZtQ6JZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahZvx4XpQ6JZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGYxuCR3Q6JZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stD5OunxQ6Jg"
      },
      "source": [
        "#### Epsilon-decay sample function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qc7xMkRQ6Jg"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Try building a similar epsilon-decay function for your model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tm6xR85Q6Jg"
      },
      "source": [
        "time = np.arange(0,10000)\n",
        "epsilon = []\n",
        "for i in range(0,10000):\n",
        "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg5jfTOwQ6Jh"
      },
      "source": [
        "plt.plot(time, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38wv7RxQ6Ji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}